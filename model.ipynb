{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Sequential\n",
    "from IPython.display import clear_output\n",
    "import util\n",
    "import encoders\n",
    "import re\n",
    "from nltk import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from random import sample\n",
    "import pickle\n",
    "import imp\n",
    "from util import Text2mat\n",
    "from encoders import Encoder, CombinedEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recurrent neural network\n",
    "\n",
    "##### one layer LSTM\n",
    "1. add dropout(worked); lstm dropout & lstm recurrent dropout & dropout layer. \n",
    "   \n",
    "   validation loss stablises near 0.20(best 0.18)\n",
    "2. add embedding size and hidden layer dimension;(x)\n",
    "3. increase the depth of network; (x)\n",
    "4. decrease learning rate; (x)\n",
    "5. add a LSTM layer; (x)\n",
    "6. use pretrained embeddings (worked);\n",
    "    validation loss decreases under 0.18\n",
    "7. use biLSTM;(x)\n",
    "8. use sentences in reversed order; (x)\n",
    "9. add self-attention; (x)\n",
    "10. add ELMO word embeddings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(util)\n",
    "imp.reload(encoders)\n",
    "\n",
    "from util import Text2mat\n",
    "from encoders import Encoder, CombinedEncoder, WordRNN, CharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train_v2.csv')\n",
    "\n",
    "labels = df.target.values\n",
    "text = df.apply(lambda row:row['text'],axis=1)\\\n",
    "          .apply(lambda line:line.replace('#','').lower())\\\n",
    "          .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = TweetTokenizer()\n",
    "# lines = [tokenizer.tokenize(line) for line in lines]\n",
    "lines = util.clean_text(text)\n",
    "\n",
    "lines_train, lines_dev, y_train, y_dev = train_test_split(lines,labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 out-of-vocabulary words.\n"
     ]
    }
   ],
   "source": [
    "# #use pretrained word embeddings\n",
    "# import gensim.downloader as api\n",
    "# pre_word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# pre_embeddings = {word:pre_word_vectors[word] for word in pre_word_vectors.vocab}\n",
    "\n",
    "with open('glove-wiki-gigaword-100-word-embeddings.pkl','rb') as f:\n",
    "    pre_embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "all_pre_embeddings = np.stack(pre_embeddings.values())\n",
    "pre_embed_mean, pre_embed_var = np.mean(all_pre_embeddings,axis=0), np.var(all_pre_embeddings,axis=0)\n",
    "\n",
    "embedding_weight_matrix = np.random.normal(pre_embed_mean,pre_embed_var,(text2mat.n_tokens,100))\n",
    "\n",
    "num_oov = 0\n",
    "for i in range(text2mat.n_tokens):\n",
    "    word = text2mat.id_to_token[i]\n",
    "    if word in pre_embeddings:\n",
    "        embedding_weight_matrix[i] = pre_embeddings[word]\n",
    "    else:\n",
    "        num_oov += 1\n",
    "print(f'{num_oov} out-of-vocabulary words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2mat_word = Text2mat(lines,low_count_threshold=2)\n",
    "text2mat_char = Text2mat(lines,mode='char',low_count_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size, hid_size = 256, 128\n",
    "vocab_size, char_size = text2mat_word.n_tokens, text2mat_char.n_tokens\n",
    "conv_kernel_sizes = [3,5,7,9]\n",
    "conv_hid_size = 64\n",
    "\n",
    "#test word + char -level word embeddings\n",
    "model = CombinedEncoder(vocab_size,char_size,emb_size,emb_size,hid_size,hid_size,\n",
    "                       conv_kernel_sizes,conv_hid_size)\n",
    "\n",
    "# #test only word rnn\n",
    "# model = WRNN(vocab_size,emb_size,hid_size)\n",
    "\n",
    "# #test attention encoder\n",
    "# model = newEncoder(vocab_size,emb_size,hid_size,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.04130412]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_logits(['i awf fgd'],text2mat_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shape verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input_w = text2mat_word.to_matrix(lines[:3])\n",
    "dummy_input_c = text2mat_char.to_matrix(lines[:3])\n",
    "print(f'input word:{dummy_input_w.shape}')\n",
    "print(f'input char:{dummy_input_c.shape}')\n",
    "\n",
    "word_encoder_output = model.w_encoder(dummy_input_w)\n",
    "print(f'word encoder:{word_encoder_output.shape}')\n",
    "\n",
    "char_encoder_output = model.c_encoder(dummy_input_c)\n",
    "print(f'word encoder:{char_encoder_output.shape}')\n",
    "\n",
    "total_output = model(dummy_input_w,dummy_input_c)\n",
    "print(f'final output:{total_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model,batch_lines,batch_label):\n",
    "    logits = model.get_logits(batch_lines,text2mat_word,text2mat_char)\n",
    "    labels = tf.cast(tf.reshape(batch_label,[-1,1]),logits.dtype)\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels,logits))\n",
    "\n",
    "def validation_loss(model,dev_lines,dev_label,batch_size=64):\n",
    "    total_loss = 0\n",
    "    N = len(dev_lines)\n",
    "    step = 0\n",
    "    for i in range(0,N,batch_size):\n",
    "        dev_line, dev_y = dev_lines[i:i+batch_size], dev_label[i:i+batch_size]\n",
    "        total_loss += compute_loss(model,dev_line,dev_y)\n",
    "        step += 1\n",
    "    return total_loss / step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatch(X,y,batch_size=64,cycle=True):\n",
    "    while True:\n",
    "        N = len(X)\n",
    "        #shuffle at the beginning of each round\n",
    "        indices = np.arange(N)\n",
    "        np.random.shuffle(indices)\n",
    "        X = [X[i] for i in indices]\n",
    "        \n",
    "        for i in range(0,N,batch_size):\n",
    "            batch_X = X[i:i+batch_size]\n",
    "            batch_y = y[i:i+batch_size]\n",
    "            yield batch_X, batch_y\n",
    "        \n",
    "        if not cycle:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 train loss:(0, 0.6807831)\n",
      "EPOCH 0 dev loss:(0, 0.6583183)\n",
      "EPOCH 1 train loss:(1, 0.529605)\n",
      "EPOCH 1 dev loss:(1, 0.5164897)\n",
      "EPOCH 2 train loss:(2, 0.3588581)\n",
      "EPOCH 2 dev loss:(2, 0.51208913)\n",
      "EPOCH 3 train loss:(3, 0.26165175)\n",
      "EPOCH 3 dev loss:(3, 0.5357696)\n",
      "EPOCH 4 train loss:(4, 0.20656405)\n",
      "EPOCH 4 dev loss:(4, 0.65574425)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-51330cc94f64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m#print(batch_line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mloss_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_line\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_i\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-8df8ad337b6a>\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(model, batch_lines, batch_label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_lines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_lines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext2mat_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext2mat_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pythonfiles\\kaggle\\NLP with Disaster Tweets\\encoders.py\u001b[0m in \u001b[0;36mget_logits\u001b[1;34m(self, batch_lines, transformer_word, transformer_char)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mbatch_ixs_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mbatch_ixs_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_char\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ixs_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_ixs_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pythonfiles\\kaggle\\NLP with Disaster Tweets\\encoders.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X_w, X_c)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mX_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mX_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mX_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_char\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\pythonfiles\\kaggle\\NLP with Disaster Tweets\\encoders.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, test)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    415\u001b[0m           \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow_lengths\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow_lengths\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m           \u001b[0mtime_major\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m           zero_output_for_mask=self.zero_output_for_mask)\n\u001b[0m\u001b[0;32m    418\u001b[0m       \u001b[1;31m# This is a dummy tensor for testing purpose.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m       \u001b[0mruntime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_RUNTIME_UNKNOWN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[0;32m   4172\u001b[0m           \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4173\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4174\u001b[1;33m           **while_loop_kwargs)\n\u001b[0m\u001b[0;32m   4175\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[0;32m   2713\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2714\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m   4156\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4157\u001b[0m         output, new_states = step_function(current_input,\n\u001b[1;32m-> 4158\u001b[1;33m                                            tuple(states) + tuple(constants))\n\u001b[0m\u001b[0;32m   4159\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4160\u001b[0m         \u001b[0mflat_new_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m       last_output, outputs, states = K.rnn(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, states, training)\u001b[0m\n\u001b[0;32m   1823\u001b[0m         \u001b[0mrecurrent_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_tm1_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1824\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1825\u001b[1;33m           \u001b[0mrecurrent_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurrent_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1826\u001b[0m         \u001b[0mrecurrent_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrecurrent_h\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m   9499\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9500\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9501\u001b[1;33m         \"new_axis_mask\", new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m   9502\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9503\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "NUM_OF_EXAMPLE = len(lines_train)\n",
    "train_indices = list(np.arange(len(lines_train)))\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "train_history = []\n",
    "dev_history = []\n",
    "\n",
    "best_dev_loss = np.inf\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #shuffle at the beginning of each round\n",
    "    np.random.shuffle(train_indices)\n",
    "    lines_train = [lines_train[i] for i in train_indices]\n",
    "    y_train = [y_train[i] for i in train_indices]\n",
    "\n",
    "    epoch_losses = []\n",
    "    \n",
    "    step = 1\n",
    "    for i in range(0,NUM_OF_EXAMPLE,BATCH_SIZE):\n",
    "        batch_line = lines_train[i:i+BATCH_SIZE]\n",
    "        batch_label = y_train[i:i+BATCH_SIZE]\n",
    "        \n",
    "        batch_size_step = len(batch_line)\n",
    "        \n",
    "        #print(f'EPOCH {epoch} STEP {step} TRAIN ON {batch_size_step} EXAMPLES.')\n",
    "    \n",
    "        #print(batch_line)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_i = compute_loss(model,batch_line,batch_label)\n",
    "    \n",
    "        grads = tape.gradient(loss_i,model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        \n",
    "        #print(sum(sum(model.get_weights()[0])))\n",
    "        \n",
    "        epoch_losses.append(loss_i.numpy())\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    train_history.append((epoch,np.mean(epoch_losses)))\n",
    "    dev_loss_i = validation_loss(model,lines_dev,y_dev,BATCH_SIZE)\n",
    "    dev_history.append((epoch,dev_loss_i.numpy()))\n",
    "    \n",
    "    print(f'EPOCH {epoch} train loss:{train_history[-1]}')\n",
    "    print(f'EPOCH {epoch} dev loss:{dev_history[-1]}')\n",
    "#     clear_output(wait=True)\n",
    "    \n",
    "#     plt.figure(figsize=[12, 6])\n",
    "#     plt.subplot(1,2,1), plt.title('train loss'), plt.grid()\n",
    "#     plt.scatter(*zip(*train_history),alpha=0.5,color='blue')\n",
    "    \n",
    "#     plt.subplot(1,2,2), plt.title('dev loss'), plt.grid()\n",
    "#     plt.scatter(*zip(*dev_history),color='orange')\n",
    "        \n",
    "#     plt.show()\n",
    "        \n",
    "    #restore from the best \n",
    "    if dev_loss_i < best_dev_loss:\n",
    "        best_weights = model.get_weights()\n",
    "        best_dev_loss = dev_loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [('ab',3),('a',10),('a',3),('b',1),('ab',0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', ' asd', 'ac']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ['',' asd','ac']\n",
    "sorted(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"acv\".find('c',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model reset\n",
    "keras.backend.clear_session()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 500\n",
    "patience = 5\n",
    "optimizer = tf.optimizers.Adam()\n",
    "train_size = list(np.arange(len(lines_train)))\n",
    "\n",
    "train_history = []\n",
    "dev_history = []\n",
    "\n",
    "best_dev_loss = np.inf\n",
    "data_generator = generate_minibatch(lines_train,y_train,BATCH_SIZE)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    batch_line, batch_label = next(data_generator)\n",
    "    batch_size_step = len(batch_line)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_i = compute_loss(model,batch_line,batch_label)\n",
    "    \n",
    "    grads = tape.gradient(loss_i,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    \n",
    "    train_history.append((i,loss_i.numpy()))\n",
    "    if i % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        dev_loss_i = validation_loss(model,lines_dev,y_dev,BATCH_SIZE)\n",
    "        dev_history.append((i,dev_loss_i))\n",
    "\n",
    "        print(f'train on {batch_size_step} examples.')\n",
    "        \n",
    "        plt.figure(figsize=[12, 6])\n",
    "        plt.subplot(1,2,1), plt.title('train loss'), plt.grid()\n",
    "        plt.scatter(*zip(*train_history),alpha=0.5,color='blue')\n",
    "        \n",
    "        plt.subplot(1,2,2), plt.title('validation loss'), plt.grid()\n",
    "        plt.plot(*zip(*dev_history),color='orange')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        #restore from the best\n",
    "        if dev_loss_i < best_dev_loss:\n",
    "            best_weights = model.get_weights()\n",
    "            best_dev_loss = dev_loss_i\n",
    "        \n",
    "#restore from the best iteration\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,lines_to_eva,y,batch_size=256):\n",
    "    pred = np.array([])\n",
    "    for i in range(0,len(lines_to_eva),batch_size):\n",
    "        batch_line = lines_to_eva[i:i+batch_size]\n",
    "        X_w, X_c = text2mat_word.to_matrix(batch_line), text2mat_char.to_matrix(batch_line)\n",
    "        pred_i = np.rint(tf.math.sigmoid(model(X_w,X_c))).astype('int').reshape((-1,))\n",
    "        pred = np.concatenate([pred,pred_i])\n",
    "    return classification_report(y,pred)\n",
    "#     pos_truth, pos_pred = y.sum(), pred.sum()\n",
    "#     true_positive = (pred == y)[y == 1].sum()\n",
    "#     return true_positive / pos_pred, true_positive / pos_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81       845\n",
      "           1       0.77      0.71      0.73       642\n",
      "\n",
      "    accuracy                           0.78      1487\n",
      "   macro avg       0.78      0.77      0.77      1487\n",
      "weighted avg       0.78      0.78      0.78      1487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model,lines_dev,y_dev,64))\n",
    "# p, r = evaluate(model,lines_dev,y_dev)\n",
    "# print(f\"precision : {p}\")\n",
    "# print(f\"recall : {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,lines_to_test,batch_size=256):\n",
    "    pred = []\n",
    "    for i in range(0,len(lines_to_test),batch_size):\n",
    "        X = text2mat.to_matrix(lines_to_test[i:i+batch_size])\n",
    "        pred_i = np.rint(tf.math.sigmoid(model(X))).astype('int').reshape((-1,))\n",
    "        pred.extend(list(pred_i))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(model,lines_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leicester_merc : icymi - ashes 2015: australia collapse at trent bridge - how twitter rea\\x89û_  '"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrrr = re.compile('@\\S*|http:\\S*|https:\\S*|http\\S*|http://*S*')\n",
    "re.sub(rrrr,\"\",\"leicester_merc : icymi - ashes 2015: australia collapse at trent bridge - how twitter reaû_ http://t.co/hqewmreyso) http://t.co/y4y8fcljed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "text_test = df_test.apply(lambda row:row['text'],axis=1)\\\n",
    "          .apply(lambda line:line.replace('#','').lower())\\\n",
    "          .tolist()\n",
    "\n",
    "lines_test = [re.sub(pattern,'',line).strip() for line in text_test]\n",
    "tokenizer = TweetTokenizer()\n",
    "lines_test = [tokenizer.tokenize(line) for line in lines_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(model,lines_test,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_submit = pd.DataFrame({'id':df_test.id.values,'target':pred})\n",
    "\n",
    "df_to_submit.to_csv('submission_rnn_v2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline model: Naive Bayes\n",
    "\n",
    "- use english stopwords\n",
    "\n",
    "- filter rare words with document count < 2(not important)\n",
    "\n",
    "- tf-idf feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# df = pd.read_csv('./data/train_v1.csv')\n",
    "\n",
    "# y = df.target.values\n",
    "\n",
    "# lines = df.apply(lambda row:row['text'],axis=1).tolist()\n",
    "# tfidf_vectorizer = TfidfVectorizer(min_df=2,stop_words='english')\n",
    "# X = tfidf_vectorizer.fit_transform(lines)\n",
    "\n",
    "# clf = MultinomialNB()\n",
    "# clf.fit(X,y)\n",
    "\n",
    "# df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# lines_test = df_test.apply(lambda row:row['text'],axis=1).tolist()\n",
    "# X_test = tfidf_vectorizer.transform(lines_test)\n",
    "\n",
    "# pred = clf.predict(X_test)\n",
    "\n",
    "# df_to_submit = pd.DataFrame({'id':df_test.id.values,'target':pred})\n",
    "\n",
    "# df_to_submit.to_csv('submission_nb.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
